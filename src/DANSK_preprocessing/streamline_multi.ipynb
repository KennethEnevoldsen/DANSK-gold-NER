{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dacy\n",
    "import os\n",
    "import json\n",
    "import copy\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from spacy.tokens import DocBin, Doc, Span\n",
    "from spacy.training.corpus import Corpus\n",
    "from itertools import combinations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for retrieving indexes of duplicates\n",
    "def duplicate_indexes_in_list(listt):\n",
    "    listt = [i[\"ent.text\"] for i in listt]\n",
    "    print(listt)\n",
    "    duplicate_items = [item for item, count in Counter(listt).items() if count > 1]\n",
    "\n",
    "    return {\n",
    "        item: [i for i, e in enumerate(listt) if e == item]\n",
    "        for item in duplicate_items\n",
    "    }\n",
    "    \n",
    "# Define function for removing the duplicates with the lowest count.\n",
    "def find_idxs_for_removal(duplicate_items_and_idxs, unique_ents_count):\n",
    "    idxs_for_removal = []\n",
    "    for k, v in duplicate_items_and_idxs.items():\n",
    "        if unique_ents_count[v[0]] > unique_ents_count[v[1]]:\n",
    "            idxs_for_removal.append(v[1])\n",
    "\n",
    "        if unique_ents_count[v[0]] < unique_ents_count[v[1]]:\n",
    "            idxs_for_removal.append(v[0])\n",
    "\n",
    "        if unique_ents_count[v[0]] == unique_ents_count[v[1]]:\n",
    "            idxs_for_removal.extend((v[0], v[1]))\n",
    "    \n",
    "    return idxs_for_removal\n",
    "\n",
    "# Define function for removing list elements from a list of idx's\n",
    "def remove_list_elements_from_idxs(some_list, idxs_for_removal):\n",
    "    for i in sorted(idxs_for_removal, reverse=True):\n",
    "        del some_list[i]\n",
    "    return some_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['something', 'a', 'something', 'b', 'a', 'c']\n",
      "[{'ent.text': 'a'}, {'ent.text': 'b'}, {'ent.text': 'c'}]\n",
      "[2, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "unique_ents = [{'ent.text' : \"something\"}, {'ent.text' : \"a\"}, {'ent.text' : \"something\"}, {'ent.text' : \"b\"}, {'ent.text' : \"a\"}, {'ent.text' : \"c\"}]\n",
    "#unique_ents = [\"something\", \"a\"]\n",
    "\n",
    "unique_ents_count = [4, 2, 4, 1, 1, 1]\n",
    "#unique_ents_count = [4, 2]\n",
    "\n",
    "duplicate_items_and_idxs = duplicate_indexes_in_list(unique_ents)\n",
    "idxs_for_removal = find_idxs_for_removal(duplicate_items_and_idxs, unique_ents_count)\n",
    "\n",
    "unique_ents = remove_list_elements_from_idxs(unique_ents, idxs_for_removal)\n",
    "unique_ents_count = remove_list_elements_from_idxs(unique_ents_count, idxs_for_removal)\n",
    "print(unique_ents)\n",
    "print(unique_ents_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function for retriveing all ents for a given doc\n",
    "def retrieve_all_ents(doc, all_docs):\n",
    "    ents_for_doc = []\n",
    "    # Acquire list of all ents for a doc\n",
    "    for i in all_docs:\n",
    "        if i.text == doc.text:\n",
    "            for ent in i.ents:\n",
    "                ents_for_doc.append(ent)\n",
    "    return ents_for_doc\n",
    "\n",
    "\n",
    "# Defining a function for exploding a doc and exploding its ents\n",
    "def explode_doc(doc, ents):\n",
    "    ents_exploded = [\n",
    "        {\n",
    "            \"ent\": ent,\n",
    "            \"ent.text\": ent.text,\n",
    "            \"ent.label_\": ent.label_,\n",
    "            \"ent.label_and_text\": ent.text + ent.label_,\n",
    "        }\n",
    "        for ent in ents\n",
    "    ]\n",
    "    return {\n",
    "        \"doc.text\": doc.text,\n",
    "        \"doc\": doc,\n",
    "        \"doc.ents\": ents_exploded,\n",
    "    }\n",
    "\n",
    "\n",
    "# Defining function for retrieving a list with unique ents, and the count of the unique ents\n",
    "def doc_ents_count(exploded_doc, match):\n",
    "    unique_ents = []\n",
    "    unique_ents_count = []\n",
    "    \n",
    "    if match == 'label_and_text':\n",
    "        for ent_idx in range(len(exploded_doc[\"doc.ents\"])):\n",
    "            ent = exploded_doc[\"doc.ents\"][ent_idx]\n",
    "            # If ent is in unique_ents, unique_ents_count += 1, for same index:\n",
    "            if any(\n",
    "                ent[\"ent.label_and_text\"] == unique_ent[\"ent.label_and_text\"]\n",
    "                for unique_ent in unique_ents\n",
    "            ):\n",
    "                # print(f\"\"\"ent \"{ent[\"ent.text\"]}\" already in unique_ents\"\"\")\n",
    "                unique_ent_label_and_texts = [\n",
    "                    unique_ent[\"ent.label_and_text\"] for unique_ent in unique_ents\n",
    "                ]\n",
    "                index_of_same_ent = unique_ent_label_and_texts.index(\n",
    "                    ent[\"ent.label_and_text\"]\n",
    "                )\n",
    "                unique_ents_count[index_of_same_ent] += 1\n",
    "            else:\n",
    "                # print(f\"\"\"ent \"{ent[\"ent.text\"]}\" not already in unique_ents\"\"\")\n",
    "                unique_ents.append(ent)\n",
    "                unique_ents_count.append(1)\n",
    "    \n",
    "    else:\n",
    "        for ent_idx in range(len(exploded_doc[\"doc.ents\"])):\n",
    "            ent = exploded_doc[\"doc.ents\"][ent_idx]\n",
    "            # If ent is in unique_ents, unique_ents_count += 1, for same index:\n",
    "            if any(\n",
    "                ent[\"ent.text\"] == unique_ent[\"ent.text\"]\n",
    "                for unique_ent in unique_ents\n",
    "            ):\n",
    "                # print(f\"\"\"ent \"{ent[\"ent.text\"]}\" already in unique_ents\"\"\")\n",
    "                unique_ent_texts = [\n",
    "                    unique_ent[\"ent.text\"] for unique_ent in unique_ents\n",
    "                ]\n",
    "                index_of_same_ent = unique_ent_texts.index(\n",
    "                    ent[\"ent.text\"]\n",
    "                )\n",
    "                unique_ents_count[index_of_same_ent] += 1\n",
    "            else:\n",
    "                # print(f\"\"\"ent \"{ent[\"ent.text\"]}\" not already in unique_ents\"\"\")\n",
    "                unique_ents.append(ent)\n",
    "                unique_ents_count.append(1)        \n",
    "    return exploded_doc[\"doc\"], unique_ents, unique_ents_count\n",
    "\n",
    "# Define function for retrieving indexes of duplicates\n",
    "def duplicate_indexes_in_list(listt):\n",
    "    listt = [i[\"ent.text\"] for i in listt]\n",
    "    duplicate_items = [item for item, count in Counter(listt).items() if count > 1]\n",
    "    return {\n",
    "        item: [i for i, e in enumerate(listt) if e == item]\n",
    "        for item in duplicate_items\n",
    "    }\n",
    "    \n",
    "# Define function for removing the duplicates with the lowest count.\n",
    "def find_idxs_for_removal(duplicate_items_and_idxs, unique_ents_count):\n",
    "    idxs_for_removal = []\n",
    "    for k, v in duplicate_items_and_idxs.items():\n",
    "        if unique_ents_count[v[0]] > unique_ents_count[v[1]]:\n",
    "            idxs_for_removal.append(v[1])\n",
    "\n",
    "        if unique_ents_count[v[0]] < unique_ents_count[v[1]]:\n",
    "            idxs_for_removal.append(v[0])\n",
    "\n",
    "        if unique_ents_count[v[0]] == unique_ents_count[v[1]]:\n",
    "            idxs_for_removal.extend((v[0], v[1]))\n",
    "    \n",
    "    return idxs_for_removal\n",
    "\n",
    "# Define function for removing list elements from a list of idx's\n",
    "def remove_list_elements_from_idxs(some_list, idxs_for_removal):\n",
    "    for i in sorted(idxs_for_removal, reverse=True):\n",
    "        del some_list[i]\n",
    "    return some_list\n",
    "\n",
    "# Define a function for only keeping the most frequent ent in cases of different ents for the same span\n",
    "def filter_away_duplicates_unique_ents(unique_ents, unique_ents_proportion):\n",
    "    duplicate_items = [item for item, count in Counter(unique_ents).items() if count > 1]\n",
    "    \n",
    "    return unique_ents, unique_ents_proportion\n",
    "\n",
    "# Define function for getting ratio of docs where ent appears\n",
    "def get_ratio(doc, unique_ents, unique_ents_count, all_docs, n_raters):\n",
    "    #all_doc_texts = [doc.text for doc in all_docs]\n",
    "    unique_ents_proportion = [i / n_raters for i in unique_ents_count]\n",
    "    return doc, unique_ents, unique_ents_proportion\n",
    "\n",
    "\n",
    "# Define a function for finding frequent annotations (above certain threshold)\n",
    "def retrieve_freq_or_infreq_ents(\n",
    "    doc, unique_ents, unique_ents_ratio, threshold, find_freq=True\n",
    "):\n",
    "    if find_freq == True:\n",
    "        frequent_ents_for_doc = [\n",
    "            unique_ent[\"ent\"]\n",
    "            for unique_ent, unique_ent_ratio in zip(unique_ents, unique_ents_ratio)\n",
    "            if unique_ent_ratio >= threshold\n",
    "        ]\n",
    "        return doc, frequent_ents_for_doc\n",
    "\n",
    "    if find_freq == False:\n",
    "        infrequent_ents_for_doc = [\n",
    "            unique_ent[\"ent\"]\n",
    "            for unique_ent, unique_ent_ratio in zip(unique_ents, unique_ents_ratio)\n",
    "            if unique_ent_ratio <= threshold\n",
    "        ]\n",
    "        return doc, infrequent_ents_for_doc\n",
    "\n",
    "# Define a function for deleting any ents in a doc that exist in the same span as a frequent ent\n",
    "def del_ents_from_freq(doc, frequent_ent_for_doc):\n",
    "    # Find indexes of doc.ents where either the start- or end character is the same as for the frequent entity\n",
    "    idxs_of_removable_ents = [\n",
    "        idx\n",
    "        for idx, item in enumerate(list(doc.ents))\n",
    "        if (\n",
    "            item.start_char == frequent_ent_for_doc.start_char\n",
    "            or item.end_char == frequent_ent_for_doc.end_char\n",
    "        )\n",
    "    ]\n",
    "    # Remove doc.ents with those indices\n",
    "    doc_ents = list(doc.ents)\n",
    "    for idx in sorted(idxs_of_removable_ents, reverse=True):\n",
    "        del doc_ents[idx]\n",
    "    doc.ents = tuple(doc_ents)\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Define a function for deleting any ents in a doc that exists in any of the same spans as a list of frequent ents\n",
    "def del_ents_from_freq_multiple(doc, frequent_ents_for_doc):\n",
    "    for frequent_ent_for_doc in frequent_ents_for_doc:\n",
    "        doc = del_ents_from_freq(doc, frequent_ent_for_doc)\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Define a function for adding a frequent entity to a doc\n",
    "def add_freq_ent_to_doc(doc, frequent_ent_for_a_doc):\n",
    "    new_doc_ents = doc.ents + (frequent_ent_for_a_doc,)\n",
    "    doc.ents = new_doc_ents\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Define a function for adding frequent ents in a list of ents to a doc\n",
    "def add_freq_ents_to_doc(doc, frequent_ents_for_a_doc):\n",
    "    for frequent_ent_for_a_doc in frequent_ents_for_a_doc:\n",
    "        doc = add_freq_ent_to_doc(doc, frequent_ent_for_a_doc)\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Define a function for finding the index of a list, in which the doc matches another doc\n",
    "def get_same_doc_index(doc, list_of_docs):\n",
    "    for i, e in enumerate(list_of_docs):\n",
    "        if e.text == doc.text:\n",
    "            return i\n",
    "\n",
    "\n",
    "# Define a function for streamlining a doc in accordance with frequent_ents_for_doc and infrequent_ents_for_doc\n",
    "def streamline(rater_doc, infrequent_ents_for_doc, frequent_ents_for_doc):\n",
    "    r = copy.deepcopy(rater_doc)\n",
    "    # Delete all entities in the doc that has the same span as the infrequent entities\n",
    "    r = del_ents_from_freq_multiple(\n",
    "        r, infrequent_ents_for_doc\n",
    "    )\n",
    "    print(frequent_ents_for_doc)\n",
    "    # Delete all entities in the doc that has the same span as the frequent entities\n",
    "    r = del_ents_from_freq_multiple(\n",
    "        r, frequent_ents_for_doc\n",
    "    )\n",
    "    print(r)\n",
    "    # Add all frequent entities to the doc\n",
    "    r = add_freq_ents_to_doc(\n",
    "        r, frequent_ents_for_doc\n",
    "    )\n",
    "    return r\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a function for the entire streamlining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def streamline_doc_for_rater(doc, flat_list, rater_docs, thresholds, n_raters):\n",
    "    # Retrieve all annotations across raters_idx_idx_idx for a doc\n",
    "    ents = retrieve_all_ents(doc, flat_list)\n",
    "\n",
    "    # Add all entities to doc, and \"explode\" the doc (dictionary format, with all relevant info)\n",
    "    exploded_doc = explode_doc(doc, ents)\n",
    "\n",
    "    # Get a count of all unique ents\n",
    "    doc, unique_ents_full_with_dupli, unique_ents_count_full_with_dupli = doc_ents_count(exploded_doc, match = 'label_and_text')\n",
    "    doc, unique_ents_full, unique_ents_count_full = doc_ents_count(exploded_doc, match = 'label_and_text')\n",
    "\n",
    "    # Get the ratio of occurrence of all unique entities with duplicates for printed output\n",
    "    doc, unique_ents_full_with_dupli, unique_ents_ratio_full_with_dupli = get_ratio(\n",
    "        doc, unique_ents_full_with_dupli, unique_ents_count_full_with_dupli, flat_list, n_raters\n",
    "    )\n",
    "    \n",
    "    print(unique_ents_full)\n",
    "\n",
    "    # Get a dictionary with keys = duplicate ent.texts, and idx, the index that these duplicates have in unique_ents and unique_ents_count\n",
    "    duplicate_items_and_idxs = duplicate_indexes_in_list(unique_ents_full)\n",
    "    \n",
    "    # Find indexes of unique_ents_count and unique_ents that should be deleted (in cases where there are)\n",
    "    idxs_for_removal = find_idxs_for_removal(duplicate_items_and_idxs, unique_ents_count_full)\n",
    "\n",
    "    unique_ents_full = remove_list_elements_from_idxs(unique_ents_full, idxs_for_removal)\n",
    "    unique_ents_count_full = remove_list_elements_from_idxs(unique_ents_count_full, idxs_for_removal)\n",
    "\n",
    "    # Get the ratio of occurrence of all unique entities\n",
    "    doc, unique_ents_full, unique_ents_ratio_full = get_ratio(\n",
    "        doc, unique_ents_full, unique_ents_count_full, flat_list, n_raters\n",
    "    )\n",
    "\n",
    "    # Retrieve the entities that are frequent across all raters\n",
    "    doc, frequent_ents_for_doc = retrieve_freq_or_infreq_ents(\n",
    "        doc, unique_ents_full, unique_ents_ratio_full, threshold=thresholds['find_freq'], find_freq=True\n",
    "    )\n",
    "    \n",
    "    # Get a count of all unique ents\n",
    "    doc, unique_ents_partial, unique_ents_count_partial = doc_ents_count(exploded_doc, match = 'text')\n",
    "\n",
    "    # Get the ratio of occurrence of all unique entities\n",
    "    doc, unique_ents_partial, unique_ents_ratio_partial = get_ratio(\n",
    "        doc, unique_ents_partial, unique_ents_count_partial, flat_list, n_raters\n",
    "    )\n",
    "\n",
    "    # Retrieve the entities that are infrequent across all raters\n",
    "    doc, infrequent_ents_for_doc = retrieve_freq_or_infreq_ents(\n",
    "        doc, unique_ents_partial, unique_ents_ratio_partial, threshold=thresholds['find_infreq'], find_freq=False\n",
    "    )\n",
    "    \n",
    "    unique_ents_full_texts_with_dupli = [ent['ent.text'] for ent in unique_ents_full_with_dupli]\n",
    "    unique_ents_partial_texts_with_dupli = [ent['ent.text'] for ent in unique_ents_partial]\n",
    "    \n",
    "    # Get index of the doc in question\n",
    "    idx = get_same_doc_index(doc, rater_docs)\n",
    "\n",
    "    print(f'Doc index for rater: {idx} \\nDoc: {doc.text} \\nunique_ents_full_with_dupli: {unique_ents_full_texts_with_dupli} \\nunique_ents_ratio_full_with_dupli: {unique_ents_ratio_full_with_dupli} \\nfrequent_ents: {frequent_ents_for_doc} \\nunique_ents_partial_with_dupli: {unique_ents_partial_texts_with_dupli} \\nunique_ents_ratio_partial: {unique_ents_ratio_partial} \\ninfrequent_ents: {infrequent_ents_for_doc}')\n",
    "\n",
    "    # If the doc exists in the raters data\n",
    "    if idx is not None:\n",
    "        # Retrieve the doc that should be streamlined\n",
    "        rater_doc = copy.deepcopy(rater_docs[idx])\n",
    "        print(f'doc.ents BEFORE streamlining: {rater_doc.ents}')\n",
    "        # Streamline the doc\n",
    "        streamlined_rater_doc = streamline(rater_doc, infrequent_ents_for_doc, frequent_ents_for_doc)\n",
    "        print(f'doc.ents AFTER streamlining: {streamlined_rater_doc.ents} \\n\\n\\n\\n')\n",
    "        return streamlined_rater_doc\n",
    "    \n",
    "    else:\n",
    "        print(\"\\n\\n\\n\\ndoc not in rater_docs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emiltrencknerjessen/miniconda3/envs/thesis/lib/python3.10/site-packages/spacy/util.py:833: UserWarning: [W095] Model 'da_dacy_medium_trf' (0.1.0) was trained with spaCy v3.1 and may not be 100% compatible with the current version (3.2.4). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/emiltrencknerjessen/miniconda3/envs/thesis/lib/python3.10/site-packages/spacy_transformers/pipeline_component.py:406: UserWarning: Automatically converting a transformer component from spacy-transformers v1.0 to v1.1+. If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spacy-transformers version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# Change cwd\n",
    "os.chdir(\"/Users/emiltrencknerjessen/Desktop/priv/DANSK-gold-NER\")\n",
    "\n",
    "# Load language object\n",
    "nlp = dacy.load(\"medium\")\n",
    "\n",
    "# List relevant data and sort by rater number\n",
    "data_paths = glob.glob(\"./data/DANSK-multi/unprocessed/rater*/data.spacy\")\n",
    "data_paths.sort()\n",
    "data_paths.sort(key=\"./data/DANSK-multi/unprocessed/rater_10/data.spacy\".__eq__)\n",
    "\n",
    "# Load in data and get rater indices (if not already loaded)\n",
    "data = []\n",
    "raters_idx = []\n",
    "for path in data_paths:\n",
    "    # Get rater indices\n",
    "    rater_idx = re.search(r\"\\d+\", path).group()\n",
    "    raters_idx.append(int(rater_idx) - 1)\n",
    "\n",
    "    # Load data\n",
    "    doc_bin = DocBin().from_disk(path)\n",
    "    docs = list(doc_bin.get_docs(nlp.vocab))[:20]\n",
    "    data.append(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "token0 = data[6][1].ents[0]\n",
    "token1 = data[5][1].ents[0]\n",
    "token2 = data[0][7].ents[0]\n",
    "tokens = [token0, token1, token2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#dkpol 22 28\n",
      "dkpol 23 28\n",
      "dk 155 157\n"
     ]
    }
   ],
   "source": [
    "print(token0, token0.start_char, token0.end_char)\n",
    "print(token1, token1.start_char, token1.end_char)\n",
    "print(token2, token2.start_char, token2.end_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for determining whether two tokens occupy parts of the same span\n",
    "def is_token_contained_within_other_token(token0, token1):\n",
    "    # Return False if one isn't contained in the other, and True if it is\n",
    "    return token1.start_char <= token0.start_char and token1.end_char >= token0.end_char or token0.start_char <= token1.start_char and token0.end_char >= token1.end_char\n",
    "\n",
    "# Function for getting entities in a list of entities that occupy each others spans\n",
    "def get_same_span_ents(ents_list):\n",
    "    freq_ent_same_spans = []\n",
    "    a = combinations(tokens, 2)\n",
    "    for i in a:\n",
    "        if is_token_contained_within_other_token(i[0], i[1]):\n",
    "            freq_ent_same_spans.extend(i)\n",
    "    return freq_ent_same_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[#dkpol, dkpol]"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_same_span_ents(tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excluding rater 2, 8 and 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluding rater 2, 7 and 10\n",
    "indexes = [1, 7, 9]\n",
    "for index in sorted(indexes, reverse=True):\n",
    "    del data[index]\n",
    "\n",
    "raters_idx = raters_idx[:7]\n",
    "\n",
    "raters_lookup = {0: 1, 1: 3, 2: 4, 3: 5, 4: 6, 5: 7, 6: 9}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve unique documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all unique docs\n",
    "unique_docs = []\n",
    "flat_list = [item for sublist in data for item in sublist]\n",
    "for doc in flat_list:\n",
    "    if all(doc.text != unique_doc.text for unique_doc in unique_docs):\n",
    "        unique_docs.append(copy.deepcopy(doc))\n",
    "        \n",
    "# Ensure that unique_docs don't already have entities\n",
    "for i in unique_docs:\n",
    "    i.ents = ()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streamlining docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a doc and a list of all docs (with duplicates from each rater)\n",
    "# doc = unique_docs[0]\n",
    "# flat_list = flat_list\n",
    "# rater_docs = copy.deepcopy(data[0])\n",
    "# thresholds = {'find_freq': .5, 'find_infreq': .4}\n",
    "\n",
    "# A single doc for a single rater\n",
    "#streamlined_rater_doc = streamline_doc_for_rater(doc, flat_list, rater_docs, thresholds)\n",
    "\n",
    "# All docs for a single rater\n",
    "# streamlined_docs = []\n",
    "# for doc in unique_docs:\n",
    "#     streamlined_docs.append(streamline_doc_for_rater(doc, flat_list, rater_docs, thresholds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current rater idx: 0\n",
      "Current rater: 1\n",
      "[{'ent': danske, 'ent.text': 'danske', 'ent.label_': 'NORP', 'ent.label_and_text': 'danskeNORP'}, {'ent': statsministeren, 'ent.text': 'statsministeren', 'ent.label_': 'PERSON', 'ent.label_and_text': 'statsministerenPERSON'}, {'ent': hospitaler, 'ent.text': 'hospitaler', 'ent.label_': 'FACILITY', 'ent.label_and_text': 'hospitalerFACILITY'}, {'ent': indvandrerdrenge, 'ent.text': 'indvandrerdrenge', 'ent.label_': 'NORP', 'ent.label_and_text': 'indvandrerdrengeNORP'}]\n",
      "Doc index for rater: 0 \n",
      "Doc: Hvordan kan statsministeren kalde børn,der er født på danske hospitaler for indvandrerdrenge! \n",
      "unique_ents_full_with_dupli: ['danske', 'statsministeren', 'hospitaler', 'indvandrerdrenge'] \n",
      "unique_ents_ratio_full_with_dupli: [1.0, 0.2857142857142857, 0.2857142857142857, 0.2857142857142857] \n",
      "frequent_ents: [danske, statsministeren, hospitaler, indvandrerdrenge] \n",
      "unique_ents_partial_with_dupli: ['danske', 'statsministeren', 'hospitaler', 'indvandrerdrenge'] \n",
      "unique_ents_ratio_partial: [1.0, 0.2857142857142857, 0.2857142857142857, 0.2857142857142857] \n",
      "infrequent_ents: [statsministeren, hospitaler, indvandrerdrenge]\n",
      "doc.ents BEFORE streamlining: (danske,)\n",
      "[danske, statsministeren, hospitaler, indvandrerdrenge]\n",
      "Hvordan kan statsministeren kalde børn,der er født på danske hospitaler for indvandrerdrenge!\n",
      "doc.ents AFTER streamlining: (statsministeren, danske, hospitaler, indvandrerdrenge) \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current rater idx: 0\n",
      "Current rater: 1\n",
      "[{'ent': dkpol, 'ent.text': 'dkpol', 'ent.label_': 'ORGANIZATION', 'ent.label_and_text': 'dkpolORGANIZATION'}, {'ent': #dkpol, 'ent.text': '#dkpol', 'ent.label_': 'NORP', 'ent.label_and_text': '#dkpolNORP'}]\n",
      "Doc index for rater: 1 \n",
      "Doc: Det her er jo håbløst #dkpol  https://t.co/e7hAg155Gw \n",
      "unique_ents_full_with_dupli: ['dkpol', '#dkpol'] \n",
      "unique_ents_ratio_full_with_dupli: [0.14285714285714285, 0.14285714285714285] \n",
      "frequent_ents: [dkpol, #dkpol] \n",
      "unique_ents_partial_with_dupli: ['dkpol', '#dkpol'] \n",
      "unique_ents_ratio_partial: [0.14285714285714285, 0.14285714285714285] \n",
      "infrequent_ents: [dkpol, #dkpol]\n",
      "doc.ents BEFORE streamlining: ()\n",
      "[dkpol, #dkpol]\n",
      "Det her er jo håbløst #dkpol  https://t.co/e7hAg155Gw\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[E1010] Unable to set entity information for token 6 which is included in more than one span in entities, blocked, missing or outside.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[250], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mCurrent rater: \u001b[39m\u001b[39m{\u001b[39;00mraters_lookup[rater_idx]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[39m#if streamlined_doc_for_rater:=streamline_doc_for_rater(doc, flat_list, rater_docs, thresholds) != None:\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m streamlined_doc \u001b[39m=\u001b[39m streamline_doc_for_rater(doc, flat_list, rater_docs, thresholds, n_raters)\n\u001b[1;32m     16\u001b[0m \u001b[39mif\u001b[39;00m streamlined_doc \u001b[39m!=\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     streamlined_docs\u001b[39m.\u001b[39mappend(streamlined_doc)\n",
      "Cell \u001b[0;32mIn[241], line 65\u001b[0m, in \u001b[0;36mstreamline_doc_for_rater\u001b[0;34m(doc, flat_list, rater_docs, thresholds, n_raters)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdoc.ents BEFORE streamlining: \u001b[39m\u001b[39m{\u001b[39;00mrater_doc\u001b[39m.\u001b[39ments\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     64\u001b[0m \u001b[39m# Streamline the doc\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m streamlined_rater_doc \u001b[39m=\u001b[39m streamline(rater_doc, infrequent_ents_for_doc, frequent_ents_for_doc)\n\u001b[1;32m     66\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdoc.ents AFTER streamlining: \u001b[39m\u001b[39m{\u001b[39;00mstreamlined_rater_doc\u001b[39m.\u001b[39ments\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     67\u001b[0m \u001b[39mreturn\u001b[39;00m streamlined_rater_doc\n",
      "Cell \u001b[0;32mIn[168], line 202\u001b[0m, in \u001b[0;36mstreamline\u001b[0;34m(rater_doc, infrequent_ents_for_doc, frequent_ents_for_doc)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[39mprint\u001b[39m(r)\n\u001b[1;32m    201\u001b[0m \u001b[39m# Add all frequent entities to the doc\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m r \u001b[39m=\u001b[39m add_freq_ents_to_doc(\n\u001b[1;32m    203\u001b[0m     r, frequent_ents_for_doc\n\u001b[1;32m    204\u001b[0m )\n\u001b[1;32m    205\u001b[0m \u001b[39mreturn\u001b[39;00m r\n",
      "Cell \u001b[0;32mIn[168], line 177\u001b[0m, in \u001b[0;36madd_freq_ents_to_doc\u001b[0;34m(doc, frequent_ents_for_a_doc)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madd_freq_ents_to_doc\u001b[39m(doc, frequent_ents_for_a_doc):\n\u001b[1;32m    176\u001b[0m     \u001b[39mfor\u001b[39;00m frequent_ent_for_a_doc \u001b[39min\u001b[39;00m frequent_ents_for_a_doc:\n\u001b[0;32m--> 177\u001b[0m         doc \u001b[39m=\u001b[39m add_freq_ent_to_doc(doc, frequent_ent_for_a_doc)\n\u001b[1;32m    178\u001b[0m     \u001b[39mreturn\u001b[39;00m doc\n",
      "Cell \u001b[0;32mIn[168], line 170\u001b[0m, in \u001b[0;36madd_freq_ent_to_doc\u001b[0;34m(doc, frequent_ent_for_a_doc)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madd_freq_ent_to_doc\u001b[39m(doc, frequent_ent_for_a_doc):\n\u001b[1;32m    169\u001b[0m     new_doc_ents \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39ments \u001b[39m+\u001b[39m (frequent_ent_for_a_doc,)\n\u001b[0;32m--> 170\u001b[0m     doc\u001b[39m.\u001b[39;49ments \u001b[39m=\u001b[39m new_doc_ents\n\u001b[1;32m    171\u001b[0m     \u001b[39mreturn\u001b[39;00m doc\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.10/site-packages/spacy/tokens/doc.pyx:750\u001b[0m, in \u001b[0;36mspacy.tokens.doc.Doc.ents.__set__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.10/site-packages/spacy/tokens/doc.pyx:787\u001b[0m, in \u001b[0;36mspacy.tokens.doc.Doc.set_ents\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: [E1010] Unable to set entity information for token 6 which is included in more than one span in entities, blocked, missing or outside."
     ]
    }
   ],
   "source": [
    "# All docs for all raters\n",
    "flat_list = flat_list\n",
    "thresholds = {'find_freq': 1/7, 'find_infreq': 2/7}\n",
    "n_raters = len(raters_idx)\n",
    "\n",
    "streamlined_data = []\n",
    "for rater_idx in raters_idx:\n",
    "    \n",
    "    streamlined_docs = []\n",
    "    rater_docs = copy.deepcopy(data[rater_idx])\n",
    "    for doc in unique_docs:\n",
    "        print(f'Current rater idx: {rater_idx}')\n",
    "        print(f'Current rater: {raters_lookup[rater_idx]}')\n",
    "        #if streamlined_doc_for_rater:=streamline_doc_for_rater(doc, flat_list, rater_docs, thresholds) != None:\n",
    "        streamlined_doc = streamline_doc_for_rater(doc, flat_list, rater_docs, thresholds, n_raters)\n",
    "        if streamlined_doc != None:\n",
    "            streamlined_docs.append(streamlined_doc)\n",
    "    streamlined_data.append(streamlined_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all streamlined docs as jsonl\n",
    "for rater_idx in raters_idx:\n",
    "    db = DocBin()\n",
    "    savepath = f\"./data/DANSK-multi/streamlined/rater_{raters_lookup[rater_idx]}/data.jsonl\"\n",
    "    for doc in streamlined_data[rater_idx]:\n",
    "        db.add(doc)\n",
    "    examples = []\n",
    "    for doc in db.get_docs(nlp.vocab):\n",
    "        spans = [{\"start\": ent.start_char, \"end\": ent.end_char, \"label\": ent.label_} for ent in doc.ents]\n",
    "        examples.append({\"text\": doc.text, \"spans\": spans})\n",
    "    with open(savepath, 'w') as outfile:\n",
    "        for entry in examples:\n",
    "            json.dump(entry, outfile)\n",
    "            outfile.write('\\n')\n",
    "            \n",
    "# # Save all streamlined docs as DocBins\n",
    "# for rater_idx in raters_idx:\n",
    "#     db = DocBin()\n",
    "#     savepath = f\"./data/DANSK-multi/streamlined/rater_{raters_lookup[rater_idx]}/data.spacy\"\n",
    "#     for doc in streamlined_data[rater_idx]:\n",
    "#         db.add(doc)\n",
    "#     db.to_disk(savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0f092f9808d781aed1cffbee1778720285e402013b75f9f1b9e21e42cac7a28f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
