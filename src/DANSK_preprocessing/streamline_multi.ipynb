{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dacy\n",
    "import os\n",
    "import copy\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from spacy.tokens import DocBin, Doc, Span\n",
    "from spacy.training.corpus import Corpus\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function for retriveing all ents for a given doc\n",
    "def retrieve_all_ents(doc, all_docs):\n",
    "    ents_for_doc = []\n",
    "    # Acquire list of all ents for a doc\n",
    "    for i in flat_list:\n",
    "        if i.text == doc.text:\n",
    "            for ent in i.ents:\n",
    "                ents_for_doc.append(ent)\n",
    "    return ents_for_doc\n",
    "\n",
    "\n",
    "# Defining a function for exploding a doc and exploding its ents\n",
    "def explode_doc(doc, ents):\n",
    "    ents_exploded = [\n",
    "        {\n",
    "            \"ent\": ent,\n",
    "            \"ent.text\": ent.text,\n",
    "            \"ent.label_\": ent.label_,\n",
    "            \"ent.label_and_text\": ent.text + ent.label_,\n",
    "        }\n",
    "        for ent in ents\n",
    "    ]\n",
    "    return {\n",
    "        \"doc.text\": doc.text,\n",
    "        \"doc\": doc,\n",
    "        \"doc.ents\": ents_exploded,\n",
    "    }\n",
    "\n",
    "\n",
    "# Defining function for retrieving a list with unique ents, and the count of the unique ents\n",
    "def doc_ents_count(exploded_doc, match):\n",
    "    unique_ents = []\n",
    "    unique_ents_count = []\n",
    "    \n",
    "    if match == 'label_and_text':\n",
    "        for ent_idx in range(len(exploded_doc[\"doc.ents\"])):\n",
    "            ent = exploded_doc[\"doc.ents\"][ent_idx]\n",
    "            # If ent is in unique_ents, unique_ents_count += 1, for same index:\n",
    "            if any(\n",
    "                ent[\"ent.label_and_text\"] == unique_ent[\"ent.label_and_text\"]\n",
    "                for unique_ent in unique_ents\n",
    "            ):\n",
    "                # print(f\"\"\"ent \"{ent[\"ent.text\"]}\" already in unique_ents\"\"\")\n",
    "                unique_ent_label_and_texts = [\n",
    "                    unique_ent[\"ent.label_and_text\"] for unique_ent in unique_ents\n",
    "                ]\n",
    "                index_of_same_ent = unique_ent_label_and_texts.index(\n",
    "                    ent[\"ent.label_and_text\"]\n",
    "                )\n",
    "                unique_ents_count[index_of_same_ent] += 1\n",
    "            else:\n",
    "                # print(f\"\"\"ent \"{ent[\"ent.text\"]}\" not already in unique_ents\"\"\")\n",
    "                unique_ents.append(ent)\n",
    "                unique_ents_count.append(1)\n",
    "    \n",
    "    else:\n",
    "        for ent_idx in range(len(exploded_doc[\"doc.ents\"])):\n",
    "            ent = exploded_doc[\"doc.ents\"][ent_idx]\n",
    "            # If ent is in unique_ents, unique_ents_count += 1, for same index:\n",
    "            if any(\n",
    "                ent[\"ent.text\"] == unique_ent[\"ent.text\"]\n",
    "                for unique_ent in unique_ents\n",
    "            ):\n",
    "                # print(f\"\"\"ent \"{ent[\"ent.text\"]}\" already in unique_ents\"\"\")\n",
    "                unique_ent_texts = [\n",
    "                    unique_ent[\"ent.text\"] for unique_ent in unique_ents\n",
    "                ]\n",
    "                index_of_same_ent = unique_ent_texts.index(\n",
    "                    ent[\"ent.text\"]\n",
    "                )\n",
    "                unique_ents_count[index_of_same_ent] += 1\n",
    "            else:\n",
    "                # print(f\"\"\"ent \"{ent[\"ent.text\"]}\" not already in unique_ents\"\"\")\n",
    "                unique_ents.append(ent)\n",
    "                unique_ents_count.append(1)        \n",
    "    return exploded_doc[\"doc\"], unique_ents, unique_ents_count\n",
    "\n",
    "\n",
    "# Define function for getting ratio of docs where ent appears\n",
    "def get_ratio(doc, unique_ents, unique_ents_count, all_docs):\n",
    "    all_doc_texts = [doc.text for doc in all_docs]\n",
    "    # print(all_doc_texts.count(doc.text))\n",
    "    unique_ents_proportion = [i / 10 for i in unique_ents_count]\n",
    "    return doc, unique_ents, unique_ents_proportion\n",
    "\n",
    "\n",
    "# Define a function for finding frequent annotations (above certain threshold)\n",
    "def retrieve_freq_or_infreq_ents(\n",
    "    doc, unique_ents, unique_ents_ratio, threshold=0.5, find_freq=True\n",
    "):\n",
    "    if find_freq == True:\n",
    "        frequent_ents_for_doc = [\n",
    "            unique_ent[\"ent\"]\n",
    "            for unique_ent, unique_ent_ratio in zip(unique_ents, unique_ents_ratio)\n",
    "            if unique_ent_ratio > threshold\n",
    "        ]\n",
    "        return doc, frequent_ents_for_doc\n",
    "\n",
    "    if find_freq == False:\n",
    "        infrequent_ents_for_doc = [\n",
    "            unique_ent[\"ent\"]\n",
    "            for unique_ent, unique_ent_ratio in zip(unique_ents, unique_ents_ratio)\n",
    "            if unique_ent_ratio < threshold\n",
    "        ]\n",
    "        return doc, infrequent_ents_for_doc\n",
    "\n",
    "\n",
    "# Define a function for deleting any ents in a doc that exist in the same span as a frequent ent\n",
    "def del_ents_from_freq(doc, frequent_ent_for_doc):\n",
    "    # Find indexes of doc.ents where either the start- or end character is the same as for the frequent entity\n",
    "    idxs_of_removable_ents = [\n",
    "        idx\n",
    "        for idx, item in enumerate(list(doc.ents))\n",
    "        if (\n",
    "            item.start_char == frequent_ent_for_doc.start_char\n",
    "            or item.end_char == frequent_ent_for_doc.end_char\n",
    "        )\n",
    "    ]\n",
    "    # Remove doc.ents with those indices\n",
    "    doc_ents = list(doc.ents)\n",
    "    for idx in sorted(idxs_of_removable_ents, reverse=True):\n",
    "        del doc_ents[idx]\n",
    "    doc.ents = tuple(doc_ents)\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Define a function for deleting any ents in a doc that exists in any of the same spans as a list of frequent ents\n",
    "def del_ents_from_freq_multiple(doc, frequent_ents_for_doc):\n",
    "    for frequent_ent_for_doc in frequent_ents_for_doc:\n",
    "        doc = del_ents_from_freq(doc, frequent_ent_for_doc)\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Define a function for adding a frequent entity to a doc\n",
    "def add_freq_ent_to_doc(doc, frequent_ent_for_a_doc):\n",
    "    new_doc_ents = doc.ents + (frequent_ent_for_a_doc,)\n",
    "    doc.ents = new_doc_ents\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Define a function for adding frequent ents in a list of ents to a doc\n",
    "def add_freq_ents_to_doc(doc, frequent_ents_for_a_doc):\n",
    "    for frequent_ent_for_a_doc in frequent_ents_for_a_doc:\n",
    "        doc = add_freq_ent_to_doc(doc, frequent_ent_for_a_doc)\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Define a function for finding the index of a list, in which the doc matches another doc\n",
    "def get_same_doc_index(doc, list_of_docs):\n",
    "    for i, e in enumerate(list_of_docs):\n",
    "        if e.text == doc.text:\n",
    "            return i\n",
    "\n",
    "\n",
    "# Define a function for streamlining a doc in accordance with frequent_ents_for_doc and infrequent_ents_for_doc\n",
    "def streamline(rater_doc, infrequent_ents_for_doc, frequent_ents_for_doc):\n",
    "    r = copy.deepcopy(rater_doc)\n",
    "    # Delete all entities in the doc that has the same span as the infrequent entities\n",
    "    r = del_ents_from_freq_multiple(\n",
    "        r, infrequent_ents_for_doc\n",
    "    )\n",
    "    # Delete all entities in the doc that has the same span as the frequent entities\n",
    "    r = del_ents_from_freq_multiple(\n",
    "        r, frequent_ents_for_doc\n",
    "    )\n",
    "    # Add all frequent entities to the doc\n",
    "    r = add_freq_ents_to_doc(\n",
    "        r, frequent_ents_for_doc\n",
    "    )\n",
    "    return r\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a function for the entire streamlining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def streamline_doc_for_rater(doc, flat_list, rater_docs, thresholds):\n",
    "    # Retrieve all annotations across raters for a doc\n",
    "    ents = retrieve_all_ents(doc, flat_list)\n",
    "\n",
    "    # Add all entities to doc, and \"explode\" the doc (dictionary format, with all relevant info)\n",
    "    exploded_doc = explode_doc(doc, ents)\n",
    "\n",
    "    # Get a count of all unique ents\n",
    "    doc, unique_ents_full, unique_ents_count_full = doc_ents_count(exploded_doc, match = 'label_and_text')\n",
    "\n",
    "    # Get the ratio of occurrence of all unique entities\n",
    "    doc, unique_ents_full, unique_ents_ratio_full = get_ratio(\n",
    "        doc, unique_ents_full, unique_ents_count_full, flat_list\n",
    "    )\n",
    "\n",
    "    # Retrieve the entities that are frequent across all raters\n",
    "    doc, frequent_ents_for_doc = retrieve_freq_or_infreq_ents(\n",
    "        doc, unique_ents_full, unique_ents_ratio_full, threshold=thresholds['find_freq'], find_freq=True\n",
    "    )\n",
    "\n",
    "    # Get a count of all unique ents\n",
    "    doc, unique_ents_partial, unique_ents_count_partial = doc_ents_count(exploded_doc, match = 'text')\n",
    "\n",
    "    # Get the ratio of occurrence of all unique entities\n",
    "    doc, unique_ents_partial, unique_ents_ratio_partial = get_ratio(\n",
    "        doc, unique_ents_partial, unique_ents_count_partial, flat_list\n",
    "    )\n",
    "\n",
    "    # Retrieve the entities that are infrequent across all raters\n",
    "    doc, infrequent_ents_for_doc = retrieve_freq_or_infreq_ents(\n",
    "        doc, unique_ents_partial, unique_ents_ratio_partial, threshold=thresholds['find_infreq'], find_freq=False\n",
    "    )\n",
    "    \n",
    "    unique_ents_full_texts = [ent['ent.text'] for ent in unique_ents_full]\n",
    "    unique_ents_partial_texts = [ent['ent.text'] for ent in unique_ents_partial]\n",
    "    \n",
    "    # Get index of the doc in question\n",
    "    idx = get_same_doc_index(doc, rater_docs)\n",
    "\n",
    "    print(f'Doc index for rater: {idx} \\nDoc: {doc.text} \\nunique_ents_full: {unique_ents_full_texts} \\nunique_ents_ratio_full: {unique_ents_ratio_full} \\nfrequent_ents: {frequent_ents_for_doc} \\nunique_ents_partial: {unique_ents_partial_texts} \\nunique_ents_ratio_partial: {unique_ents_ratio_partial} \\ninfrequent_ents: {infrequent_ents_for_doc}')\n",
    "\n",
    "    # If the doc exists in the raters data\n",
    "    if idx is not None:\n",
    "        # Retrieve the doc that should be streamlined\n",
    "        rater_doc = copy.deepcopy(rater_docs[idx])\n",
    "        print(f'doc.ents BEFORE streamlining: {rater_doc.ents}')\n",
    "        # Streamline the doc\n",
    "        streamlined_rater_doc = streamline(rater_doc, infrequent_ents_for_doc, frequent_ents_for_doc)\n",
    "        print(f'doc.ents AFTER streamlining: {streamlined_rater_doc.ents} \\n\\n\\n\\n')\n",
    "        return streamlined_rater_doc\n",
    "    \n",
    "    else:\n",
    "        print(\"\\n\\n\\n\\ndoc not in rater_docs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emiltrencknerjessen/miniconda3/envs/thesis/lib/python3.10/site-packages/spacy/util.py:833: UserWarning: [W095] Model 'da_dacy_medium_trf' (0.1.0) was trained with spaCy v3.1 and may not be 100% compatible with the current version (3.2.4). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/emiltrencknerjessen/miniconda3/envs/thesis/lib/python3.10/site-packages/spacy_transformers/pipeline_component.py:406: UserWarning: Automatically converting a transformer component from spacy-transformers v1.0 to v1.1+. If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spacy-transformers version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# Change cwd\n",
    "os.chdir(\"/Users/emiltrencknerjessen/Desktop/priv/DANSK-gold-NER\")\n",
    "\n",
    "# Load language object\n",
    "nlp = dacy.load(\"medium\")\n",
    "\n",
    "# List relevant data and sort by rater number\n",
    "data_paths = glob.glob(\"./data/DANSK-multi/unprocessed/rater*/data.spacy\")\n",
    "data_paths.sort()\n",
    "data_paths.sort(key=\"./data/DANSK-multi/unprocessed/rater_10/data.spacy\".__eq__)\n",
    "\n",
    "# Load in data and get rater indices (if not already loaded)\n",
    "data = []\n",
    "raters = []\n",
    "for path in data_paths:\n",
    "    # Get rater indices\n",
    "    rater = re.search(r\"\\d+\", path).group()\n",
    "    raters.append(int(rater) - 1)\n",
    "\n",
    "    # Load data\n",
    "    doc_bin = DocBin().from_disk(path)\n",
    "    docs = list(doc_bin.get_docs(nlp.vocab))[:20]\n",
    "    data.append(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Abdi, Tarik, Muhammad, Eyman)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][12].ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(danske,)\n",
      "(statsministeren, børn, danske, hospitaler, indvandrerdrenge)\n",
      "(statsministeren, danske, hospitaler)\n",
      "(statsministeren, danske, hospitaler, indvandrerdrenge)\n",
      "(danske,)\n",
      "(danske,)\n",
      "(danske, indvandrerdrenge)\n",
      "(statsministeren, børn, danske, hospitaler, indvandrerdrenge)\n",
      "(danske,)\n",
      "(danske,)\n"
     ]
    }
   ],
   "source": [
    "for i in raters:\n",
    "    print(data[i][0].ents)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve unique documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all unique docs\n",
    "unique_docs = []\n",
    "flat_list = [item for sublist in data for item in sublist]\n",
    "for doc in flat_list:\n",
    "    if all(doc.text != unique_doc.text for unique_doc in unique_docs):\n",
    "        unique_docs.append(copy.deepcopy(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that unique_docs don't already have entities\n",
    "for i in unique_docs:\n",
    "    i.ents = ()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streamlining a single doc for a single rater"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting all necessary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a doc and a list of all docs (with duplicates from each rater)\n",
    "doc = unique_docs[0]\n",
    "flat_list = flat_list\n",
    "rater_docs = copy.deepcopy(data[0])\n",
    "thresholds = {'find_freq': .3, 'find_infreq': .3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(danske,)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rater_docs[0].ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A single doc for a single rater\n",
    "#streamlined_rater_doc = streamline_doc_for_rater(doc, flat_list, rater_docs, thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All docs for a single rater\n",
    "# streamlined_docs = []\n",
    "# for doc in unique_docs:\n",
    "#     streamlined_docs.append(streamline_doc_for_rater(doc, flat_list, rater_docs, thresholds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not spacy.tokens.doc.Doc",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[158], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m rater \u001b[39min\u001b[39;00m raters:\n\u001b[1;32m      8\u001b[0m     streamlined_docs \u001b[39m=\u001b[39m []\n\u001b[0;32m----> 9\u001b[0m     rater_docs \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(data[i])\n\u001b[1;32m     10\u001b[0m     \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m unique_docs:\n\u001b[1;32m     11\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mCurrent rater: \u001b[39m\u001b[39m{\u001b[39;00mrater\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not spacy.tokens.doc.Doc"
     ]
    }
   ],
   "source": [
    "# All docs for all raters\n",
    "flat_list = flat_list\n",
    "thresholds = {'find_freq': .3, 'find_infreq': .3}\n",
    "\n",
    "streamlined_data = []\n",
    "for rater in raters:\n",
    "    \n",
    "    streamlined_docs = []\n",
    "    rater_docs = copy.deepcopy(data[rater])\n",
    "    for doc in unique_docs:\n",
    "        print(f'Current rater: {rater}')\n",
    "        streamlined_docs.append(streamline_doc_for_rater(doc, flat_list, rater_docs, thresholds))\n",
    "    streamlined_data.append(streamlined_docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Issues yet to be fixed:\n",
    "In above output, the doc: *'Varme hjerter, svære kår, gode til fodbold.'*, *'fodbold'* has a ratio of freq. of 0.1. \n",
    "However, in the output it seems as if though noone has tagged *'fodbold'*\n",
    "\n",
    "1. Check if fodbold is tagged for any of the raters, when accessing the data directly just when it has been loaded in.\n",
    "2. If it does appear, check after which function the entity dissappears.\n",
    "3. If it does not appear, search for alternative explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: (danske,)\n",
      "After: (statsministeren, danske, hospitaler, indvandrerdrenge)\n",
      "\n",
      "\n",
      "Before: (statsministeren, børn, danske, hospitaler, indvandrerdrenge)\n",
      "After: (statsministeren, danske, hospitaler, indvandrerdrenge)\n",
      "\n",
      "\n",
      "Before: (statsministeren, danske, hospitaler)\n",
      "After: (statsministeren, danske, hospitaler, indvandrerdrenge)\n",
      "\n",
      "\n",
      "Before: (statsministeren, danske, hospitaler, indvandrerdrenge)\n",
      "After: (statsministeren, danske, hospitaler, indvandrerdrenge)\n",
      "\n",
      "\n",
      "Before: (danske,)\n",
      "After: (statsministeren, danske, hospitaler, indvandrerdrenge)\n",
      "\n",
      "\n",
      "Before: (danske,)\n",
      "After: (statsministeren, danske, hospitaler, indvandrerdrenge)\n",
      "\n",
      "\n",
      "Before: (danske, indvandrerdrenge)\n",
      "After: (statsministeren, danske, hospitaler, indvandrerdrenge)\n",
      "\n",
      "\n",
      "Before: (statsministeren, børn, danske, hospitaler, indvandrerdrenge)\n",
      "After: (statsministeren, danske, hospitaler, indvandrerdrenge)\n",
      "\n",
      "\n",
      "Before: (danske,)\n",
      "After: (statsministeren, danske, hospitaler, indvandrerdrenge)\n",
      "\n",
      "\n",
      "Before: (danske,)\n",
      "After: (statsministeren, danske, hospitaler, indvandrerdrenge)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in raters:\n",
    "    print(f'Before: {data[i][0].ents}')\n",
    "    print(f'After: {streamlined_data[i][0].ents}\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save all streamlined docs as DocBins\n",
    "# for rater in raters:\n",
    "#     db = DocBin()\n",
    "#     savepath = f\"./data/DANSK-multi/streamlined/rater_{rater+1}\"\n",
    "#     for doc in data[rater]:\n",
    "#         db.add(doc)\n",
    "#     db.to_disk(savepath)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0f092f9808d781aed1cffbee1778720285e402013b75f9f1b9e21e42cac7a28f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
