{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dacy\n",
    "import os\n",
    "import copy\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from spacy.tokens import DocBin, Doc, Span\n",
    "from spacy.training.corpus import Corpus\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function for retriveing all ents for a given doc\n",
    "def retrieve_all_ents(doc, all_docs):\n",
    "    ents_for_doc = []\n",
    "    # Acquire list of all ents for a doc\n",
    "    for i in flat_list:\n",
    "        if i.text == doc.text:\n",
    "            for ent in i.ents:\n",
    "                ents_for_doc.append(ent)\n",
    "    return ents_for_doc\n",
    "\n",
    "\n",
    "# Defining a function for exploding a doc and exploding its ents\n",
    "def explode_doc(doc, ents):\n",
    "    ents_exploded = [\n",
    "        {\n",
    "            \"ent\": ent,\n",
    "            \"ent.text\": ent.text,\n",
    "            \"ent.label_\": ent.label_,\n",
    "            \"ent.label_and_text\": ent.text + ent.label_,\n",
    "        }\n",
    "        for ent in ents\n",
    "    ]\n",
    "    return {\n",
    "        \"doc.text\": doc.text,\n",
    "        \"doc\": doc,\n",
    "        \"doc.ents\": ents_exploded,\n",
    "    }\n",
    "\n",
    "\n",
    "# Defining function for retrieving a list with unique ents, and the count of the unique ents\n",
    "def doc_ents_count(exploded_doc, match):\n",
    "    unique_ents = []\n",
    "    unique_ents_count = []\n",
    "    \n",
    "    if match == 'label_and_text':\n",
    "        for ent_idx in range(len(exploded_doc[\"doc.ents\"])):\n",
    "            ent = exploded_doc[\"doc.ents\"][ent_idx]\n",
    "            # If ent is in unique_ents, unique_ents_count += 1, for same index:\n",
    "            if any(\n",
    "                ent[\"ent.label_and_text\"] == unique_ent[\"ent.label_and_text\"]\n",
    "                for unique_ent in unique_ents\n",
    "            ):\n",
    "                # print(f\"\"\"ent \"{ent[\"ent.text\"]}\" already in unique_ents\"\"\")\n",
    "                unique_ent_label_and_texts = [\n",
    "                    unique_ent[\"ent.label_and_text\"] for unique_ent in unique_ents\n",
    "                ]\n",
    "                index_of_same_ent = unique_ent_label_and_texts.index(\n",
    "                    ent[\"ent.label_and_text\"]\n",
    "                )\n",
    "                unique_ents_count[index_of_same_ent] += 1\n",
    "            else:\n",
    "                # print(f\"\"\"ent \"{ent[\"ent.text\"]}\" not already in unique_ents\"\"\")\n",
    "                unique_ents.append(ent)\n",
    "                unique_ents_count.append(1)\n",
    "    \n",
    "    else:\n",
    "        for ent_idx in range(len(exploded_doc[\"doc.ents\"])):\n",
    "            ent = exploded_doc[\"doc.ents\"][ent_idx]\n",
    "            # If ent is in unique_ents, unique_ents_count += 1, for same index:\n",
    "            if any(\n",
    "                ent[\"ent.text\"] == unique_ent[\"ent.text\"]\n",
    "                for unique_ent in unique_ents\n",
    "            ):\n",
    "                # print(f\"\"\"ent \"{ent[\"ent.text\"]}\" already in unique_ents\"\"\")\n",
    "                unique_ent_texts = [\n",
    "                    unique_ent[\"ent.text\"] for unique_ent in unique_ents\n",
    "                ]\n",
    "                index_of_same_ent = unique_ent_texts.index(\n",
    "                    ent[\"ent.text\"]\n",
    "                )\n",
    "                unique_ents_count[index_of_same_ent] += 1\n",
    "            else:\n",
    "                # print(f\"\"\"ent \"{ent[\"ent.text\"]}\" not already in unique_ents\"\"\")\n",
    "                unique_ents.append(ent)\n",
    "                unique_ents_count.append(1)        \n",
    "    return exploded_doc[\"doc\"], unique_ents, unique_ents_count\n",
    "\n",
    "\n",
    "# Define function for getting ratio of docs where ent appears\n",
    "def get_ratio(doc, unique_ents, unique_ents_count, all_docs):\n",
    "    all_doc_texts = [doc.text for doc in all_docs]\n",
    "    # print(all_doc_texts.count(doc.text))\n",
    "    unique_ents_proportion = [i / 10 for i in unique_ents_count]\n",
    "    return doc, unique_ents, unique_ents_proportion\n",
    "\n",
    "\n",
    "# Define a function for finding frequent annotations (above certain threshold)\n",
    "def retrieve_freq_or_infreq_ents(\n",
    "    doc, unique_ents, unique_ents_ratio, threshold=0.5, find_freq=True\n",
    "):\n",
    "    if find_freq == True:\n",
    "        frequent_ents_for_doc = [\n",
    "            unique_ent[\"ent\"]\n",
    "            for unique_ent, unique_ent_ratio in zip(unique_ents, unique_ents_ratio)\n",
    "            if unique_ent_ratio > threshold\n",
    "        ]\n",
    "        return doc, frequent_ents_for_doc\n",
    "\n",
    "    if find_freq == False:\n",
    "        infrequent_ents_for_doc = [\n",
    "            unique_ent[\"ent\"]\n",
    "            for unique_ent, unique_ent_ratio in zip(unique_ents, unique_ents_ratio)\n",
    "            if unique_ent_ratio < threshold\n",
    "        ]\n",
    "        return doc, infrequent_ents_for_doc\n",
    "\n",
    "\n",
    "# Define a function for deleting any ents in a doc that exist in the same span as a frequent ent\n",
    "def del_ents_from_freq(doc, frequent_ent_for_doc):\n",
    "    # Find indexes of doc.ents where either the start- or end character is the same as for the frequent entity\n",
    "    idxs_of_removable_ents = [\n",
    "        idx\n",
    "        for idx, item in enumerate(list(doc.ents))\n",
    "        if (\n",
    "            item.start_char == frequent_ent_for_doc.start_char\n",
    "            or item.end_char == frequent_ent_for_doc.end_char\n",
    "        )\n",
    "    ]\n",
    "    # Remove doc.ents with those indices\n",
    "    doc_ents = list(doc.ents)\n",
    "    for idx in sorted(idxs_of_removable_ents, reverse=True):\n",
    "        del doc_ents[idx]\n",
    "    doc.ents = tuple(doc_ents)\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Define a function for deleting any ents in a doc that exists in any of the same spans as a list of frequent ents\n",
    "def del_ents_from_freq_multiple(doc, frequent_ents_for_doc):\n",
    "    for frequent_ent_for_doc in frequent_ents_for_doc:\n",
    "        doc = del_ents_from_freq(doc, frequent_ent_for_doc)\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Define a function for adding a frequent entity to a doc\n",
    "def add_freq_ent_to_doc(doc, frequent_ent_for_a_doc):\n",
    "    new_doc_ents = doc.ents + (frequent_ent_for_a_doc,)\n",
    "    doc.ents = new_doc_ents\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Define a function for adding frequent ents in a list of ents to a doc\n",
    "def add_freq_ents_to_doc(doc, frequent_ents_for_a_doc):\n",
    "    for frequent_ent_for_a_doc in frequent_ents_for_a_doc:\n",
    "        doc = add_freq_ent_to_doc(doc, frequent_ent_for_a_doc)\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Define a function for finding the index of a list, in which the doc matches another doc\n",
    "def get_same_doc_index(doc, list_of_docs):\n",
    "    for i, e in enumerate(list_of_docs):\n",
    "        if e.text == doc.text:\n",
    "            return i\n",
    "\n",
    "\n",
    "# Define a function for streamlining a doc in accordance with frequent_ents_for_doc and infrequent_ents_for_doc\n",
    "def streamline(rater_doc, infrequent_ents_for_doc, frequent_ents_for_doc):\n",
    "    r = copy.deepcopy(rater_doc)\n",
    "    # Delete all entities in the doc that has the same span as the infrequent entities\n",
    "    r = del_ents_from_freq_multiple(\n",
    "        r, infrequent_ents_for_doc\n",
    "    )\n",
    "    # Delete all entities in the doc that has the same span as the frequent entities\n",
    "    r = del_ents_from_freq_multiple(\n",
    "        r, frequent_ents_for_doc\n",
    "    )\n",
    "    # Add all frequent entities to the doc\n",
    "    r = add_freq_ents_to_doc(\n",
    "        r, frequent_ents_for_doc\n",
    "    )\n",
    "    return r\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a function for the entire streamlining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "f-string: unmatched '[' (2805783519.py, line 34)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[432], line 34\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(f'Doc: {doc.text} \\nunique_ents_full: {[ent['ent.text'] for ent in unique_ents_full]} \\nunique_ents_ratio_full: {unique_ents_ratio_full} \\nfrequent_ents: {frequent_ents_for_doc} \\nunique_ents_partial: {unique_ents_partial} \\nunique_ents_ratio_partial: {unique_ents_ratio_partial} \\ninfrequent_ents: {infrequent_ents_for_doc}')\u001b[0m\n\u001b[0m                                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m f-string: unmatched '['\n"
     ]
    }
   ],
   "source": [
    "def streamline_doc_for_rater(doc, flat_list, rater_docs, thresholds):\n",
    "    # Retrieve all annotations across raters for a doc\n",
    "    ents = retrieve_all_ents(doc, flat_list)\n",
    "\n",
    "    # Add all entities to doc, and \"explode\" the doc (dictionary format, with all relevant info)\n",
    "    exploded_doc = explode_doc(doc, ents)\n",
    "\n",
    "    # Get a count of all unique ents\n",
    "    doc, unique_ents_full, unique_ents_count_full = doc_ents_count(exploded_doc, match = 'label_and_text')\n",
    "\n",
    "    # Get the ratio of occurrence of all unique entities\n",
    "    doc, unique_ents_full, unique_ents_ratio_full = get_ratio(\n",
    "        doc, unique_ents_full, unique_ents_count_full, flat_list\n",
    "    )\n",
    "\n",
    "    # Retrieve the entities that are frequent across all raters\n",
    "    doc, frequent_ents_for_doc = retrieve_freq_or_infreq_ents(\n",
    "        doc, unique_ents_full, unique_ents_ratio_full, threshold=thresholds['find_freq'], find_freq=True\n",
    "    )\n",
    "\n",
    "    # Get a count of all unique ents\n",
    "    doc, unique_ents_partial, unique_ents_count_partial = doc_ents_count(exploded_doc, match = 'text')\n",
    "\n",
    "    # Get the ratio of occurrence of all unique entities\n",
    "    doc, unique_ents_partial, unique_ents_ratio_partial = get_ratio(\n",
    "        doc, unique_ents_partial, unique_ents_count_partial, flat_list\n",
    "    )\n",
    "\n",
    "    # Retrieve the entities that are infrequent across all raters\n",
    "    doc, infrequent_ents_for_doc = retrieve_freq_or_infreq_ents(\n",
    "        doc, unique_ents_partial, unique_ents_ratio_partial, threshold=thresholds['find_infreq'], find_freq=False\n",
    "    )\n",
    "    \n",
    "    unique_ents_full_texts = [ent['ent.text'] for ent in unique_ents_full]\n",
    "    unique_ents_partial_texts = [ent['ent.text'] for ent in unique_ents_partial]\n",
    "    \n",
    "    print(f'Doc: {doc.text} \\nunique_ents_full: {unique_ents_full_texts} \\nunique_ents_ratio_full: {unique_ents_ratio_full} \\nfrequent_ents: {frequent_ents_for_doc} \\nunique_ents_partial: {unique_ents_partial_texts} \\nunique_ents_ratio_partial: {unique_ents_ratio_partial} \\ninfrequent_ents: {infrequent_ents_for_doc}')\n",
    "\n",
    "    # Get index of the doc in question\n",
    "    idx = get_same_doc_index(doc, rater_docs)\n",
    "\n",
    "    # If the doc exists in the raters data\n",
    "    if idx is not None:\n",
    "        # Retrieve the doc that should be streamlined\n",
    "        rater_doc = copy.deepcopy(rater_docs[idx])\n",
    "        print(f'doc.ents BEFORE streamlining: {rater_doc.ents}')\n",
    "        # Streamline the doc\n",
    "        streamlined_rater_doc = streamline(rater_doc, infrequent_ents_for_doc, frequent_ents_for_doc)\n",
    "        print(f'doc.ents AFTER streamlining: {streamlined_rater_doc.ents} \\n\\n\\n\\n')\n",
    "        return streamlined_rater_doc\n",
    "    \n",
    "    else:\n",
    "        print(\"doc not in rater_docs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emiltrencknerjessen/miniconda3/envs/thesis/lib/python3.10/site-packages/spacy/util.py:833: UserWarning: [W095] Model 'da_dacy_medium_trf' (0.1.0) was trained with spaCy v3.1 and may not be 100% compatible with the current version (3.2.4). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/emiltrencknerjessen/miniconda3/envs/thesis/lib/python3.10/site-packages/spacy_transformers/pipeline_component.py:406: UserWarning: Automatically converting a transformer component from spacy-transformers v1.0 to v1.1+. If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spacy-transformers version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# Change cwd\n",
    "os.chdir(\"/Users/emiltrencknerjessen/Desktop/priv/DANSK-gold-NER\")\n",
    "\n",
    "# Load language object\n",
    "nlp = dacy.load(\"medium\")\n",
    "\n",
    "# List relevant data and sort by rater number\n",
    "data_paths = glob.glob(\"./data/DANSK-multi/unprocessed/rater*/data.spacy\")\n",
    "data_paths.sort()\n",
    "data_paths.sort(key=\"./data/DANSK-multi/unprocessed/rater_10/data.spacy\".__eq__)\n",
    "\n",
    "# Load in data and get rater indices (if not already loaded)\n",
    "data = []\n",
    "raters = []\n",
    "for path in data_paths:\n",
    "    # Get rater indices\n",
    "    rater = re.search(r\"\\d+\", path).group()\n",
    "    raters.append(int(rater) - 1)\n",
    "\n",
    "    # Load data\n",
    "    doc_bin = DocBin().from_disk(path)\n",
    "    docs = list(doc_bin.get_docs(nlp.vocab))[:20]\n",
    "    data.append(docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve unique documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all unique docs\n",
    "unique_docs = []\n",
    "flat_list = [item for sublist in data for item in sublist]\n",
    "for doc in flat_list:\n",
    "    if all(doc.text != unique_doc.text for unique_doc in unique_docs):\n",
    "        unique_docs.append(doc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streamlining a single doc for a single rater"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting all necessary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a doc and a list of all docs (with duplicates from each rater)\n",
    "doc = unique_docs[0]\n",
    "flat_list = flat_list\n",
    "rater_docs = copy.deepcopy(data[3])\n",
    "thresholds = {'find_freq': .3, 'find_infreq': .3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "danskeNORP\n",
      "statsministerenPERSON\n",
      "børnPERSON\n",
      "hospitalerORGANIZATION\n",
      "indvandrerdrengeNORP\n",
      "hospitalerFACILITY\n",
      "indvandrerdrengePERSON\n",
      "[10, 4, 2, 2, 3, 2, 1]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "danskeNORP\n",
      "statsministerenPERSON\n",
      "børnPERSON\n",
      "hospitalerORGANIZATION\n",
      "indvandrerdrengeNORP\n",
      "[10, 4, 2, 4, 4]\n",
      "[børn]\n"
     ]
    }
   ],
   "source": [
    "# Retrieve all annotations across raters for a doc\n",
    "ents = retrieve_all_ents(doc, flat_list)\n",
    "\n",
    "# Add all entities to doc, and \"explode\" the doc (dictionary format, with all relevant info)\n",
    "exploded_doc = explode_doc(doc, ents)\n",
    "\n",
    "\n",
    "# Get a count of all unique ents\n",
    "doc, unique_ents, unique_ents_count = doc_ents_count(exploded_doc, match = 'label_and_text')\n",
    "\n",
    "for i in unique_ents:\n",
    "    print(i['ent.label_and_text'])\n",
    "\n",
    "print(unique_ents_count)\n",
    "    \n",
    "print('\\n\\n\\n')\n",
    "# Get a count of all unique ents\n",
    "doc, unique_ents, unique_ents_count = doc_ents_count(exploded_doc, match = 'lbel_and_text')\n",
    "\n",
    "for i in unique_ents:\n",
    "    print(i['ent.label_and_text'])\n",
    "print(unique_ents_count)\n",
    "# print(unique_ents_count)\n",
    "    \n",
    "# print(unique_ents_count)\n",
    "\n",
    "# Get the ratio of occurrence of all unique entities\n",
    "doc, unique_ents, unique_ents_ratio = get_ratio(\n",
    "    doc, unique_ents, unique_ents_count, flat_list\n",
    ")\n",
    "\n",
    "\n",
    "# Retrieve the entities that are infrequent across all raters\n",
    "doc, infrequent_ents_for_doc = retrieve_freq_or_infreq_ents(\n",
    "    doc, unique_ents, unique_ents_ratio, threshold=thresholds['find_infreq'], find_freq=False\n",
    ")\n",
    "\n",
    "print(infrequent_ents_for_doc)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing the actual streamlining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc: Hvordan kan statsministeren kalde børn,der er født på danske hospitaler for indvandrerdrenge! \n",
      "unique_ents_full: [{'ent': danske, 'ent.text': 'danske', 'ent.label_': 'NORP', 'ent.label_and_text': 'danskeNORP'}, {'ent': statsministeren, 'ent.text': 'statsministeren', 'ent.label_': 'PERSON', 'ent.label_and_text': 'statsministerenPERSON'}, {'ent': børn, 'ent.text': 'børn', 'ent.label_': 'PERSON', 'ent.label_and_text': 'børnPERSON'}, {'ent': hospitaler, 'ent.text': 'hospitaler', 'ent.label_': 'ORGANIZATION', 'ent.label_and_text': 'hospitalerORGANIZATION'}, {'ent': indvandrerdrenge, 'ent.text': 'indvandrerdrenge', 'ent.label_': 'NORP', 'ent.label_and_text': 'indvandrerdrengeNORP'}, {'ent': hospitaler, 'ent.text': 'hospitaler', 'ent.label_': 'FACILITY', 'ent.label_and_text': 'hospitalerFACILITY'}, {'ent': indvandrerdrenge, 'ent.text': 'indvandrerdrenge', 'ent.label_': 'PERSON', 'ent.label_and_text': 'indvandrerdrengePERSON'}] \n",
      "unique_ents_ratio_full: [1.0, 0.4, 0.2, 0.2, 0.3, 0.2, 0.1] \n",
      "frequent_ents: [danske, statsministeren] \n",
      "unique_ents_partial: [{'ent': danske, 'ent.text': 'danske', 'ent.label_': 'NORP', 'ent.label_and_text': 'danskeNORP'}, {'ent': statsministeren, 'ent.text': 'statsministeren', 'ent.label_': 'PERSON', 'ent.label_and_text': 'statsministerenPERSON'}, {'ent': børn, 'ent.text': 'børn', 'ent.label_': 'PERSON', 'ent.label_and_text': 'børnPERSON'}, {'ent': hospitaler, 'ent.text': 'hospitaler', 'ent.label_': 'ORGANIZATION', 'ent.label_and_text': 'hospitalerORGANIZATION'}, {'ent': indvandrerdrenge, 'ent.text': 'indvandrerdrenge', 'ent.label_': 'NORP', 'ent.label_and_text': 'indvandrerdrengeNORP'}] \n",
      "unique_ents_ratio_partial: [1.0, 0.4, 0.2, 0.4, 0.4] \n",
      "infrequent_ents: [børn]\n",
      "doc.ents BEFORE streamlining: (statsministeren, danske, hospitaler, indvandrerdrenge)\n",
      "doc.ents AFTER streamlining: (statsministeren, danske, hospitaler, indvandrerdrenge) \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "streamlined_rater_doc = streamline_doc_for_rater(doc, flat_list, rater_docs, thresholds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streamlining all docs for a rater"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting all necessary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = flat_list\n",
    "rater_docs = copy.deepcopy(data[3])\n",
    "thresholds = {'find_freq': .3, 'find_infreq': .3}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing the actual streamlining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc: Hvordan kan statsministeren kalde børn,der er født på danske hospitaler for indvandrerdrenge! \n",
      "frequent_ents: [danske, statsministeren] \n",
      "unique_ents_count_partial: [1.0, 0.4, 0.2, 0.4, 0.4] \n",
      "infrequent_ents: [børn], \n",
      "unique_ents_count_full: [1.0, 0.4, 0.2, 0.2, 0.3, 0.2, 0.1]\n",
      "doc.ents BEFORE streamlining: (statsministeren, danske, hospitaler, indvandrerdrenge)\n",
      "doc.ents AFTER streamlining: (statsministeren, danske, hospitaler, indvandrerdrenge) \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Doc: Det her er jo håbløst #dkpol  https://t.co/e7hAg155Gw \n",
      "frequent_ents: [] \n",
      "unique_ents_count_partial: [0.2, 0.1] \n",
      "infrequent_ents: [dkpol, #dkpol], \n",
      "unique_ents_count_full: [0.1, 0.1, 0.1]\n",
      "doc.ents BEFORE streamlining: ()\n",
      "doc.ents AFTER streamlining: () \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Doc: \n",
      "@cekicozlem \n",
      "frequent_ents: [@cekicozlem] \n",
      "unique_ents_count_partial: [0.7] \n",
      "infrequent_ents: [], \n",
      "unique_ents_count_full: [0.7]\n",
      "doc.ents BEFORE streamlining: (@cekicozlem,)\n",
      "doc.ents AFTER streamlining: (@cekicozlem,) \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Doc: Da jeg voksede op ude i vollsmose, kaldte man dem 2. Generationsindvandrere. \n",
      "frequent_ents: [vollsmose, 2.] \n",
      "unique_ents_count_partial: [1.0, 0.6, 0.2, 0.1, 0.1, 0.1, 0.1, 0.1] \n",
      "infrequent_ents: [Generationsindvandrere, 2. Generationsindvandrere., 2. Generationsindvandrere, jeg, man, dem], \n",
      "unique_ents_count_full: [0.3, 0.7, 0.5, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "doc.ents BEFORE streamlining: (vollsmose, 2. Generationsindvandrere.)\n",
      "doc.ents AFTER streamlining: (vollsmose, 2.) \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Doc: Vi må så være på 3. Generation nu? \n",
      "frequent_ents: [3.] \n",
      "unique_ents_count_partial: [0.6, 0.2, 0.1, 0.1, 0.1] \n",
      "infrequent_ents: [Generation, 3. Generation, Vi, nu], \n",
      "unique_ents_count_full: [0.4, 0.1, 0.2, 0.1, 0.1, 0.1, 0.1]\n",
      "doc.ents BEFORE streamlining: ()\n",
      "doc.ents AFTER streamlining: (3.,) \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Doc: Åndssvagt ord, som ingen problemer løser. \n",
      "frequent_ents: [] \n",
      "unique_ents_count_partial: [0.1, 0.1] \n",
      "infrequent_ents: [ord, problemer], \n",
      "unique_ents_count_full: [0.1, 0.1]\n",
      "doc.ents BEFORE streamlining: ()\n",
      "doc.ents AFTER streamlining: () \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Doc: For der ér problemer blandt disse sociale lag, og dem kommer vi først rigtig til livs når vi kigger lidt bag tæppet \n",
      "frequent_ents: [] \n",
      "unique_ents_count_partial: [0.1, 0.1, 0.2, 0.1, 0.1, 0.1] \n",
      "infrequent_ents: [problemer, lag, vi, først, lidt, tæppet], \n",
      "unique_ents_count_full: [0.1, 0.1, 0.2, 0.1, 0.1, 0.1]\n",
      "doc.ents BEFORE streamlining: ()\n",
      "doc.ents AFTER streamlining: () \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Doc: Man kunne spørge sig selv, hvad dee mon ligger til grund for at så mange går ud af den kriminelle sti - som om de intet har at miste - kunne det tænkes at dk lod dem og deres forældre i stikken da de startede deres nye liv her? \n",
      "frequent_ents: [dk] \n",
      "unique_ents_count_partial: [1.0, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2, 0.1, 0.1, 0.1, 0.1] \n",
      "infrequent_ents: [Man, sig selv,, så, mange, kriminelle, de, intet, dem, deres forældre, her], \n",
      "unique_ents_count_full: [0.9, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "doc.ents BEFORE streamlining: (dk,)\n",
      "doc.ents AFTER streamlining: (dk,) \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Doc: Man kunne også spørge om det mon er os eller dem som har skabt disse parallelsamfund. \n",
      "frequent_ents: [] \n",
      "unique_ents_count_partial: [0.2, 0.1, 0.1, 0.1] \n",
      "infrequent_ents: [parallelsamfund, Man, os, dem], \n",
      "unique_ents_count_full: [0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "doc.ents BEFORE streamlining: ()\n",
      "doc.ents AFTER streamlining: () \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Doc: Hvem har besluttet at internere alle udefrakommende i disse nedslidte boligområder, vi i dag kalder for ghettoer? \n",
      "frequent_ents: [] \n",
      "unique_ents_count_partial: [0.4, 0.1, 0.2, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1] \n",
      "infrequent_ents: [udefrakommende, boligområder, i, dag, Hvem, alle udefrakommende, vi, i dag], \n",
      "unique_ents_count_full: [0.2, 0.1, 0.1, 0.2, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "doc.ents BEFORE streamlining: (udefrakommende, boligområder, ghettoer)\n",
      "doc.ents AFTER streamlining: (ghettoer,) \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Doc: Gav man dem mulighed for at bo andre steder? \n",
      "frequent_ents: [] \n",
      "unique_ents_count_partial: [0.1, 0.1, 0.1] \n",
      "infrequent_ents: [man, dem, steder], \n",
      "unique_ents_count_full: [0.1, 0.1, 0.1]\n",
      "doc.ents BEFORE streamlining: ()\n",
      "doc.ents AFTER streamlining: () \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Doc: Nå nej \n",
      "frequent_ents: [] \n",
      "unique_ents_count_partial: [] \n",
      "infrequent_ents: [], \n",
      "unique_ents_count_full: []\n",
      "doc.ents BEFORE streamlining: ()\n",
      "doc.ents AFTER streamlining: () \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Doc: Det er selvfølgelig anekdotisk, men jeg har mødt så mange gode mennesker i min barndom som både hed Abdi, Tarik, Muhammad og Eyman. \n",
      "frequent_ents: [Abdi, Tarik, Muhammad, Eyman] \n",
      "unique_ents_count_partial: [1.0, 1.0, 1.0, 1.0, 0.1, 0.1] \n",
      "infrequent_ents: [jeg, mennesker], \n",
      "unique_ents_count_full: [1.0, 1.0, 1.0, 1.0, 0.1, 0.1]\n",
      "doc.ents BEFORE streamlining: (Abdi, Tarik, Muhammad, Eyman)\n",
      "doc.ents AFTER streamlining: (Abdi, Tarik, Muhammad, Eyman) \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Doc: Varme hjerter, svære kår, gode til fodbold. \n",
      "frequent_ents: [] \n",
      "unique_ents_count_partial: [0.1] \n",
      "infrequent_ents: [fodbold], \n",
      "unique_ents_count_full: [0.1]\n",
      "doc.ents BEFORE streamlining: ()\n",
      "doc.ents AFTER streamlining: () \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Doc: Flere af dem sælger hash osv i dag. \n",
      "frequent_ents: [dag] \n",
      "unique_ents_count_partial: [0.2, 0.2, 0.4, 0.1] \n",
      "infrequent_ents: [hash, i, Flere af dem], \n",
      "unique_ents_count_full: [0.2, 0.2, 0.4, 0.1]\n",
      "doc.ents BEFORE streamlining: ()\n",
      "doc.ents AFTER streamlining: (dag,) \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Doc: Så sørgeligt og jeg nægter tesen om at det skulle være selvforskyldt \n",
      "frequent_ents: [] \n",
      "unique_ents_count_partial: [0.1] \n",
      "infrequent_ents: [jeg], \n",
      "unique_ents_count_full: [0.1]\n",
      "doc.ents BEFORE streamlining: ()\n",
      "doc.ents AFTER streamlining: () \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Doc: Deres forældre havde ingen muligheder, allerede dengang blev de set ned på i samfundet, på kommunen og i parlamentet. \n",
      "frequent_ents: [kommunen, parlamentet] \n",
      "unique_ents_count_partial: [0.1, 0.2, 0.4, 0.5, 0.1, 0.1, 0.1] \n",
      "infrequent_ents: [forældre, samfundet, dengang, Deres forældre, de], \n",
      "unique_ents_count_full: [0.1, 0.2, 0.4, 0.5, 0.1, 0.1, 0.1]\n",
      "doc.ents BEFORE streamlining: (kommunen, parlamentet)\n",
      "doc.ents AFTER streamlining: (kommunen, parlamentet) \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Doc: Svært at gro blomster direkte på beton \n",
      "frequent_ents: [] \n",
      "unique_ents_count_partial: [0.1] \n",
      "infrequent_ents: [beton], \n",
      "unique_ents_count_full: [0.1]\n",
      "doc.ents BEFORE streamlining: ()\n",
      "doc.ents AFTER streamlining: () \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Doc: \n",
      "@iNSiG9FiX @cekicozlem \n",
      "frequent_ents: [@iNSiG9FiX, @cekicozlem] \n",
      "unique_ents_count_partial: [0.9, 0.9] \n",
      "infrequent_ents: [], \n",
      "unique_ents_count_full: [0.9, 0.9]\n",
      "doc.ents BEFORE streamlining: (@iNSiG9FiX, @cekicozlem)\n",
      "doc.ents AFTER streamlining: (@iNSiG9FiX, @cekicozlem) \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Doc: Deres forældre, bedsteforældre er det vel snarere nu, de havde alle de muligheder de valgte at forfølge. \n",
      "frequent_ents: [] \n",
      "unique_ents_count_partial: [0.2, 0.2, 0.2, 0.2] \n",
      "infrequent_ents: [forældre, bedsteforældre, nu, de], \n",
      "unique_ents_count_full: [0.2, 0.2, 0.2, 0.2]\n",
      "doc.ents BEFORE streamlining: ()\n",
      "doc.ents AFTER streamlining: () \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Doc: Det er på tide man lægger offerrollen på hylden. \n",
      "frequent_ents: [] \n",
      "unique_ents_count_partial: [0.1] \n",
      "infrequent_ents: [man], \n",
      "unique_ents_count_full: [0.1]\n",
      "doc not in rater_docs\n",
      "Doc: Vi går i samme skole, har samme mulighed for uddannelse. \n",
      "frequent_ents: [] \n",
      "unique_ents_count_partial: [0.1, 0.1, 0.1] \n",
      "infrequent_ents: [Vi, skole, uddannelse], \n",
      "unique_ents_count_full: [0.1, 0.1, 0.1]\n",
      "doc not in rater_docs\n"
     ]
    }
   ],
   "source": [
    "streamlined_docs = []\n",
    "for doc in unique_docs:\n",
    "    streamlined_docs.append(streamline_doc_for_rater(doc, flat_list, rater_docs, thresholds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save all streamlined docs as DocBins\n",
    "# for rater in raters:\n",
    "#     db = DocBin()\n",
    "#     savepath = f\"./data/DANSK-multi/streamlined/rater_{rater+1}\"\n",
    "#     for doc in data[rater]:\n",
    "#         db.add(doc)\n",
    "#     db.to_disk(savepath)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0f092f9808d781aed1cffbee1778720285e402013b75f9f1b9e21e42cac7a28f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
