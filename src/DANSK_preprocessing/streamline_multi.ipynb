{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dacy\n",
    "import os\n",
    "import copy\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from spacy.tokens import DocBin, Doc, Span\n",
    "from spacy.training.corpus import Corpus\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function for retriveing all ents for a given doc\n",
    "def retrieve_all_ents(doc, all_docs):\n",
    "    ents_for_doc = []\n",
    "    # Acquire list of all ents for a doc\n",
    "    for i in flat_list:\n",
    "        if i.text == doc.text:\n",
    "            for ent in i.ents:\n",
    "                ents_for_doc.append(ent)\n",
    "    return ents_for_doc\n",
    "\n",
    "\n",
    "# Defining a function for exploding a doc and exploding its ents\n",
    "def explode_doc(doc, ents):\n",
    "    ents_exploded = [\n",
    "        {\n",
    "            \"ent\": ent,\n",
    "            \"ent.text\": ent.text,\n",
    "            \"ent.label_\": ent.label_,\n",
    "            \"ent.label_and_text\": ent.text + ent.label_,\n",
    "        }\n",
    "        for ent in ents\n",
    "    ]\n",
    "    return {\n",
    "        \"doc.text\": doc.text,\n",
    "        \"doc\": doc,\n",
    "        \"doc.ents\": ents_exploded,\n",
    "    }\n",
    "\n",
    "\n",
    "# Defining function for retrieving a list with unique ents, and the count of the unique ents\n",
    "def doc_ents_count(exploded_doc):\n",
    "    unique_ents = []\n",
    "    unique_ents_count = []\n",
    "    for ent_idx in range(len(exploded_doc[\"doc.ents\"])):\n",
    "        ent = exploded_doc[\"doc.ents\"][ent_idx]\n",
    "        # If ent is in unique_ents, unique_ents_count += 1, for same index:\n",
    "        if any(\n",
    "            ent[\"ent.label_and_text\"] == unique_ent[\"ent.label_and_text\"]\n",
    "            for unique_ent in unique_ents\n",
    "        ):\n",
    "            # print(f\"\"\"ent \"{ent[\"ent.text\"]}\" already in unique_ents\"\"\")\n",
    "            unique_ent_label_and_texts = [\n",
    "                unique_ent[\"ent.label_and_text\"] for unique_ent in unique_ents\n",
    "            ]\n",
    "            index_of_same_ent = unique_ent_label_and_texts.index(\n",
    "                ent[\"ent.label_and_text\"]\n",
    "            )\n",
    "            unique_ents_count[index_of_same_ent] += 1\n",
    "        else:\n",
    "            # print(f\"\"\"ent \"{ent[\"ent.text\"]}\" not already in unique_ents\"\"\")\n",
    "            unique_ents.append(ent)\n",
    "            unique_ents_count.append(1)\n",
    "    return exploded_doc[\"doc\"], unique_ents, unique_ents_count\n",
    "\n",
    "\n",
    "# Define function for getting ratio of docs where ent appears\n",
    "def get_ratio(doc, unique_ents, unique_ents_count, all_docs):\n",
    "    all_doc_texts = [doc.text for doc in all_docs]\n",
    "    # print(all_doc_texts.count(doc.text))\n",
    "    unique_ents_proportion = [i / 10 for i in unique_ents_count]\n",
    "    return doc, unique_ents, unique_ents_proportion\n",
    "\n",
    "\n",
    "# Define a function for finding frequent annotations (above certain threshold)\n",
    "def retrieve_freq_or_infreq_ents(\n",
    "    doc, unique_ents, unique_ents_ratio, threshold=0.5, find_freq=True\n",
    "):\n",
    "    if find_freq == True:\n",
    "        frequent_ents_for_doc = [\n",
    "            unique_ent[\"ent\"]\n",
    "            for unique_ent, unique_ent_ratio in zip(unique_ents, unique_ents_ratio)\n",
    "            if unique_ent_ratio > threshold\n",
    "        ]\n",
    "        return doc, frequent_ents_for_doc\n",
    "\n",
    "    if find_freq == False:\n",
    "        infrequent_ents_for_doc = [\n",
    "            unique_ent[\"ent\"]\n",
    "            for unique_ent, unique_ent_ratio in zip(unique_ents, unique_ents_ratio)\n",
    "            if unique_ent_ratio < threshold\n",
    "        ]\n",
    "        return doc, infrequent_ents_for_doc\n",
    "\n",
    "\n",
    "# Define a function for deleting any ents in a doc that exist in the same span as a frequent ent\n",
    "def del_ents_from_freq(doc, frequent_ent_for_doc):\n",
    "    # Find indexes of doc.ents where either the start- or end character is the same as for the frequent entity\n",
    "    idxs_of_removable_ents = [\n",
    "        idx\n",
    "        for idx, item in enumerate(list(doc.ents))\n",
    "        if (\n",
    "            item.start_char == frequent_ent_for_doc.start_char\n",
    "            or item.end_char == frequent_ent_for_doc.end_char\n",
    "        )\n",
    "    ]\n",
    "    # Remove doc.ents with those indices\n",
    "    doc_ents = list(doc.ents)\n",
    "    for idx in sorted(idxs_of_removable_ents, reverse=True):\n",
    "        del doc_ents[idx]\n",
    "    doc.ents = tuple(doc_ents)\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Define a function for deleting any ents in a doc that exists in any of the same spans as a list of frequent ents\n",
    "def del_ents_from_freq_multiple(doc, frequent_ents_for_doc):\n",
    "    for frequent_ent_for_doc in frequent_ents_for_doc:\n",
    "        doc = del_ents_from_freq(doc, frequent_ent_for_doc)\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Define a function for adding a frequent entity to a doc\n",
    "def add_freq_ent_to_doc(doc, frequent_ent_for_a_doc):\n",
    "    new_doc_ents = doc.ents + (frequent_ent_for_a_doc,)\n",
    "    doc.ents = new_doc_ents\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Define a function for adding frequent ents in a list of ents to a doc\n",
    "def add_freq_ents_to_doc(doc, frequent_ents_for_a_doc):\n",
    "    for frequent_ent_for_a_doc in frequent_ents_for_a_doc:\n",
    "        doc = add_freq_ent_to_doc(doc, frequent_ent_for_a_doc)\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Define a function for finding the index of a list, in which the doc matches another doc\n",
    "def get_same_doc_index(doc, list_of_docs):\n",
    "    for i, e in enumerate(list_of_docs):\n",
    "        if e.text == doc.text:\n",
    "            return i\n",
    "\n",
    "\n",
    "# Define a function for streamlining a doc in accordance with frequent_ents_for_doc and infrequent_ents_for_doc\n",
    "def streamline(rater_doc, infrequent_ents_for_doc, frequent_ents_for_doc):\n",
    "    r = copy.deepcopy(rater_doc)\n",
    "    # Delete all entities in the doc that has the same span as the infrequent entities\n",
    "    r = del_ents_from_freq_multiple(\n",
    "        r, infrequent_ents_for_doc\n",
    "    )\n",
    "    # Delete all entities in the doc that has the same span as the frequent entities\n",
    "    r = del_ents_from_freq_multiple(\n",
    "        r, frequent_ents_for_doc\n",
    "    )\n",
    "    # Add all frequent entities to the doc\n",
    "    r = add_freq_ents_to_doc(\n",
    "        r, frequent_ents_for_doc\n",
    "    )\n",
    "    return r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change cwd\n",
    "os.chdir(\"/Users/emiltrencknerjessen/Desktop/priv/DANSK-gold-NER\")\n",
    "\n",
    "# Load language object\n",
    "nlp = dacy.load(\"medium\")\n",
    "\n",
    "# List relevant data and sort by rater number\n",
    "data_paths = glob.glob(\"./data/DANSK-multi/unprocessed/rater*/data.spacy\")\n",
    "data_paths.sort()\n",
    "data_paths.sort(key=\"./data/DANSK-multi/unprocessed/rater_10/data.spacy\".__eq__)\n",
    "\n",
    "# Load in data and get rater indices (if not already loaded)\n",
    "data = []\n",
    "raters = []\n",
    "for path in data_paths:\n",
    "    # Get rater indices\n",
    "    rater = re.search(r\"\\d+\", path).group()\n",
    "    raters.append(int(rater) - 1)\n",
    "\n",
    "    # Load data\n",
    "    doc_bin = DocBin().from_disk(path)\n",
    "    docs = list(doc_bin.get_docs(nlp.vocab))[:20]\n",
    "    data.append(docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve unique documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all unique docs\n",
    "unique_docs = []\n",
    "flat_list = [item for sublist in data for item in sublist]\n",
    "for doc in flat_list:\n",
    "    if all(doc.text != unique_doc.text for unique_doc in unique_docs):\n",
    "        unique_docs.append(doc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streamlining a single doc for a single rater"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a function for the entire proces"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting all necessary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a doc and a list of all docs (with duplicates from each rater)\n",
    "doc = unique_docs[0]\n",
    "flat_list = flat_list\n",
    "rater_docs = copy.deepcopy(data[3])\n",
    "thresholds = {'find_freq': .3, 'find_infreq': .3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(statsministeren, danske, hospitaler, indvandrerdrenge)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rater_docs[0].ents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing the actual streamlining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(statsministeren, danske, hospitaler, indvandrerdrenge)\n"
     ]
    }
   ],
   "source": [
    "# Retrieve all annotations across raters for a doc\n",
    "ents = retrieve_all_ents(doc, flat_list)\n",
    "\n",
    "# Add all entities to doc, and \"explode\" the doc (dictionary format, with all relevant info)\n",
    "exploded_doc = explode_doc(doc, ents)\n",
    "\n",
    "# Get a count of all unique ents\n",
    "doc, unique_ents, unique_ents_count = doc_ents_count(exploded_doc)\n",
    "\n",
    "# Get the ratio of occurrence of all unique entities\n",
    "doc, unique_ents, unique_ents_ratio = get_ratio(\n",
    "    doc, unique_ents, unique_ents_count, flat_list\n",
    ")\n",
    "\n",
    "# Retrieve the entities that are frequent across all raters\n",
    "doc, frequent_ents_for_doc = retrieve_freq_or_infreq_ents(\n",
    "    doc, unique_ents, unique_ents_ratio, threshold=thresholds['find_freq'], find_freq=True\n",
    ")\n",
    "\n",
    "# Retrieve the entities that are infrequent across all raters\n",
    "doc, infrequent_ents_for_doc = retrieve_freq_or_infreq_ents(\n",
    "    doc, unique_ents, unique_ents_ratio, threshold=thresholds['find_infreq'], find_freq=False\n",
    ")\n",
    "\n",
    "# Get index of the doc in question\n",
    "idx = get_same_doc_index(doc, rater_docs)\n",
    "\n",
    "# If the doc exists in the raters data\n",
    "if idx is not None:\n",
    "    # Retrieve the doc that should be streamlined\n",
    "    rater_doc = copy.deepcopy(rater_docs[idx])\n",
    "    print(rater_doc.ents)\n",
    "    # Streamline the doc\n",
    "    streamlined_rater_doc = streamline(rater_doc, infrequent_ents_for_doc, frequent_ents_for_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hvordan kan statsministeren kalde børn,der er født på danske hospitaler for indvandrerdrenge!\n",
      "(statsministeren, danske, hospitaler, indvandrerdrenge)\n",
      "[børn, hospitaler, hospitaler, indvandrerdrenge]\n",
      "[danske, statsministeren]\n",
      "(statsministeren, danske)\n"
     ]
    }
   ],
   "source": [
    "print(doc)\n",
    "print(rater_doc.ents)\n",
    "print(infrequent_ents_for_doc)\n",
    "print(frequent_ents_for_doc)\n",
    "print(streamlined_rater_doc.ents)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streamlining all docs for a single rater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save all streamlined docs as DocBins\n",
    "# for rater in raters:\n",
    "#     db = DocBin()\n",
    "#     savepath = f\"./data/DANSK-multi/streamlined/rater_{rater+1}\"\n",
    "#     for doc in data[rater]:\n",
    "#         db.add(doc)\n",
    "#     db.to_disk(savepath)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0f092f9808d781aed1cffbee1778720285e402013b75f9f1b9e21e42cac7a28f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
